{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model_path, output_path):\n",
    "    \"\"\"\n",
    "    Load Keras model and convert it to TFLite with quantization\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the .keras model file\n",
    "        output_path: Path to save the quantized TFLite model\n",
    "    \"\"\"\n",
    "    # Step 1: Load the Keras model\n",
    "    print(\"Loading model...\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Step 2: Convert the model to TFLite format\n",
    "    print(\"Converting to TFLite...\")\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    # Step 3: Enable quantization\n",
    "    # Using dynamic range quantization (post-training quantization)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    # Additional options for quantization\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    # Step 4: Convert the model\n",
    "    print(\"Applying quantization...\")\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Step 5: Save the quantized model\n",
    "    print(f\"Saving quantized model to {output_path}\")\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    # Calculate and print model size reduction\n",
    "    original_size = os.path.getsize(model_path) / (1024 * 1024)  # Size in MB\n",
    "    quantized_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB\n",
    "    \n",
    "    print(f\"\\nModel size comparison:\")\n",
    "    print(f\"Original model size: {original_size:.2f} MB\")\n",
    "    print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "    print(f\"Size reduction: {((original_size - quantized_size) / original_size * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "model_path = \"model/mobnet_model.keras\"\n",
    "output_path = \"model/mobnet_model_quantized.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Converting to TFLite...\n",
      "Applying quantization...\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\ASUS\\AppData\\Local\\Temp\\tmpq8y46ide\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ASUS\\AppData\\Local\\Temp\\tmpq8y46ide\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving quantized model to model/mobnet_model_quantized.tflite\n",
      "\n",
      "Model size comparison:\n",
      "Original model size: 23.44 MB\n",
      "Quantized model size: 4.27 MB\n",
      "Size reduction: 81.79%\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Run quantization\n",
    "quantize_model(model_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
